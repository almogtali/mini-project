{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4657724c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T08:38:06.817134Z",
     "start_time": "2025-06-04T08:38:06.270478Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "file_path = 'questions.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f1b4236",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T08:38:07.311724Z",
     "start_time": "2025-06-04T08:38:07.309757Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install transformers==4.49.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82f6a1ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T08:38:22.425797Z",
     "start_time": "2025-06-04T08:38:07.852630Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shistikk/.conda/envs/myenv/lib/python3.11/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "2025-06-04 11:38:14.703994: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-04 11:38:16.314161: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc55d0fb440436ea4e5db13402808d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    " \n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model_path = \"microsoft/Phi-4-mini-instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ab707e",
   "metadata": {},
   "source": [
    "# Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21a318ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T08:39:59.173011Z",
     "start_time": "2025-06-04T08:39:58.972550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                         arabic\n",
      "0   1               كيف تتشكل السحب؟\n",
      "1   2        كيف انقرضت الديناصورات؟\n",
      "2   3               ما هو الديناصور؟\n",
      "3   4  لماذا تختلف ألوان عيون البشر؟\n",
      "4   5     ما الفرق بين الذئب والكلب؟\n"
     ]
    }
   ],
   "source": [
    "sheet_name = 'arabic'  # or the exact name of the sheet\n",
    "\n",
    "# Read the specific sheet\n",
    "df_a = pd.read_excel(file_path, sheet_name=sheet_name, engine='openpyxl')\n",
    "\n",
    "# Print the first few rows\n",
    "print(df_a.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3d4ba1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T08:39:59.715951Z",
     "start_time": "2025-06-04T08:39:59.703665Z"
    }
   },
   "outputs": [],
   "source": [
    "df_a = df_a.drop(columns=['id']).rename(columns={\"arabic\": \"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82f679d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T08:53:15.888590Z",
     "start_time": "2025-06-04T08:40:00.945914Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Generating phi4mini_arabic_1:   0%|          | 0/40 [00:00<?, ?it/s]/home/shistikk/.conda/envs/myenv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Generating phi4mini_arabic_1:  28%|██▊       | 11/40 [00:52<01:53,  3.93s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Generating phi4mini_arabic_1: 100%|██████████| 40/40 [04:24<00:00,  6.60s/it]\n",
      "Generating phi4mini_arabic_2:   0%|          | 0/40 [00:00<?, ?it/s]/home/shistikk/.conda/envs/myenv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Generating phi4mini_arabic_2: 100%|██████████| 40/40 [04:23<00:00,  6.59s/it]\n",
      "Generating phi4mini_arabic_3:   0%|          | 0/40 [00:00<?, ?it/s]/home/shistikk/.conda/envs/myenv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Generating phi4mini_arabic_3: 100%|██████████| 40/40 [04:27<00:00,  6.68s/it]\n"
     ]
    }
   ],
   "source": [
    "# Define the generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Generation arguments\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "# Function to generate answer for each question\n",
    "def generate_answer(q):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"أجب عن السؤال التالي في خمس إلى ست جمل. أجب على السؤال باللغة العربية.\"},\n",
    "        {\"role\": \"user\", \"content\": q},\n",
    "    ]\n",
    "    try:\n",
    "        output = pipe(messages, **generation_args)\n",
    "        return output[0]['generated_text']\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Run the generation 3 times\n",
    "for i in range(1, 4):\n",
    "    df_copy = df_a.copy()\n",
    "    tqdm.pandas(desc=f\"Generating phi4mini_arabic_{i}\")\n",
    "    df_copy['classify'] = df_copy['text'].progress_apply(generate_answer)\n",
    "    df_copy.to_csv(f\"phi4mini_arabic_{i}.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83a0b2f",
   "metadata": {},
   "source": [
    "# English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d50d66ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T08:53:19.839318Z",
     "start_time": "2025-06-04T08:53:19.605195Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0556119",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T08:53:20.832536Z",
     "start_time": "2025-06-04T08:53:20.816316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                           english\n",
      "0   1                            How are clouds formed?\n",
      "1   2             How did the dinosaurs become extinct?\n",
      "2   3                               What is a dinosaur?\n",
      "3   4          Why do humans have different eye colors?\n",
      "4   5  What is the difference between a wolf and a dog?\n"
     ]
    }
   ],
   "source": [
    "sheet_name = 'english'  # or the exact name of the sheet\n",
    "\n",
    "# Read the specific sheet\n",
    "df_e = pd.read_excel(file_path, sheet_name=sheet_name, engine='openpyxl')\n",
    "\n",
    "# Print the first few rows\n",
    "print(df_e.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e43cf949",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T08:53:22.970279Z",
     "start_time": "2025-06-04T08:53:22.967630Z"
    }
   },
   "outputs": [],
   "source": [
    "df_e = df_e.drop(columns=['id']).rename(columns={\"english\": \"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aacd1675",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T09:00:58.702554Z",
     "start_time": "2025-06-04T08:53:43.839526Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Generating phi4mini_arabic_1:   0%|          | 0/40 [00:00<?, ?it/s]/home/shistikk/.conda/envs/myenv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Generating phi4mini_arabic_1: 100%|██████████| 40/40 [02:25<00:00,  3.63s/it]\n",
      "Generating phi4mini_arabic_2:   0%|          | 0/40 [00:00<?, ?it/s]/home/shistikk/.conda/envs/myenv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Generating phi4mini_arabic_2: 100%|██████████| 40/40 [02:25<00:00,  3.63s/it]\n",
      "Generating phi4mini_arabic_3:   0%|          | 0/40 [00:00<?, ?it/s]/home/shistikk/.conda/envs/myenv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Generating phi4mini_arabic_3: 100%|██████████| 40/40 [02:24<00:00,  3.61s/it]\n"
     ]
    }
   ],
   "source": [
    "# Define the generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Generation arguments\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "# Function to generate answer for each question\n",
    "def generate_answer(q):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Answer the following question in five to six sentences.\"},\n",
    "        {\"role\": \"user\", \"content\": q},\n",
    "    ]\n",
    "    try:\n",
    "        output = pipe(messages, **generation_args)\n",
    "        return output[0]['generated_text']\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Run the generation 3 times with different output files\n",
    "for i in range(1, 4):\n",
    "    df_copy = df_e.copy()\n",
    "    tqdm.pandas(desc=f\"Generating phi4mini_arabic_{i}\")\n",
    "    df_copy['classify'] = df_copy['text'].progress_apply(generate_answer)\n",
    "    df_copy.to_csv(f\"phi4mini_english_{i}.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614122c4",
   "metadata": {},
   "source": [
    "# Hebrew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d4c06bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T09:24:25.296136Z",
     "start_time": "2025-06-04T09:24:25.065977Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec3eba4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T09:01:17.313323Z",
     "start_time": "2025-06-04T09:01:17.292432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                             hebrew\n",
      "0   1                  איך נוצרים עננים?\n",
      "1   2              איך הדינזוארים נכחדו?\n",
      "2   3                    מה זה דינוזאור?\n",
      "3   4  למה יש לבני אדם צבעי עיניים שונים\n",
      "4   5            מה ההבדל בין זאב לכלב ?\n"
     ]
    }
   ],
   "source": [
    "sheet_name = 'hebrew'  # or the exact name of the sheet\n",
    "\n",
    "# Read the specific sheet\n",
    "df_h = pd.read_excel(file_path, sheet_name=sheet_name, engine='openpyxl')\n",
    "\n",
    "# Print the first few rows\n",
    "print(df_h.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a87aa8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T09:01:20.219327Z",
     "start_time": "2025-06-04T09:01:20.216430Z"
    }
   },
   "outputs": [],
   "source": [
    "df_h = df_h.drop(columns=['id']).rename(columns={\"hebrew\": \"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd93f9c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T09:24:15.674325Z",
     "start_time": "2025-06-04T09:01:23.819224Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Generating phi4mini_arabic_1:   0%|          | 0/40 [00:00<?, ?it/s]/home/shistikk/.conda/envs/myenv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Generating phi4mini_arabic_1: 100%|██████████| 40/40 [07:32<00:00, 11.30s/it]\n",
      "Generating phi4mini_arabic_2:   0%|          | 0/40 [00:00<?, ?it/s]/home/shistikk/.conda/envs/myenv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Generating phi4mini_arabic_2: 100%|██████████| 40/40 [07:37<00:00, 11.44s/it]\n",
      "Generating phi4mini_arabic_3:   0%|          | 0/40 [00:00<?, ?it/s]/home/shistikk/.conda/envs/myenv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Generating phi4mini_arabic_3: 100%|██████████| 40/40 [07:42<00:00, 11.56s/it]\n"
     ]
    }
   ],
   "source": [
    "# Define the generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Generation arguments\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "# Function to generate answer for each question\n",
    "def generate_answer(q):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"ענה על השאלה הבאה בחמישה עד שישה משפטים.\"},\n",
    "        {\"role\": \"user\", \"content\": q},\n",
    "    ]\n",
    "    try:\n",
    "        output = pipe(messages, **generation_args)\n",
    "        return output[0]['generated_text']\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Run the generation 3 times with different output files\n",
    "for i in range(1, 4):\n",
    "    df_copy = df_h.copy()\n",
    "    tqdm.pandas(desc=f\"Generating phi4mini_arabic_{i}\")\n",
    "    df_copy['classify'] = df_copy['text'].progress_apply(generate_answer)\n",
    "    df_copy.to_csv(f\"phi4mini_hebrew_{i}.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3182bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
